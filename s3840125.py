# -*- coding: utf-8 -*-
"""A1 (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XmgiwCeL1YMGfYdBEiOJzBl0bSRsviG_
"""

# Setting up the environment and importing necessary libraries
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import tensorflow as tf
AUTOTUNE = tf.data.experimental.AUTOTUNE
import numpy as np
import pandas as pd

import pathlib
import shutil
import tempfile

from  IPython import display
from matplotlib import pyplot as plt

# Commented out IPython magic to ensure Python compatibility.
# Create directory for TensorBoard logs
logdir = pathlib.Path(tempfile.mkdtemp())/"tensorboard_logs"
shutil.rmtree(logdir, ignore_errors=True)

# Load the TensorBoard notebook extension
# %load_ext tensorboard

# Open an embedded TensorBoard viewer
# %tensorboard --logdir {logdir}/models

from google.colab import drive
drive.mount('/content/drive')

# Loading dataset labels from a CSV file
csv_path = '/content/drive/MyDrive/dataset/data_labels.csv'
df = pd.read_csv(csv_path)

# Splitting dataset into training, validation, and test sets
from sklearn.model_selection import train_test_split

# Splitting data into train (85%) and test (15%)
train_df, test_df = train_test_split(df, test_size=0.15, random_state=42)

# Further splitting train data into train (82%) and validation (18%) to achieve an overall split of 70-15-15
train_df, val_df = train_test_split(train_df, test_size=0.1765, random_state=42)

# Display the first few rows of the dataframe to understand its structure
print(df.head())

# Check the shape of the dataframe
print(f"Total number of records: {df.shape[0]}")
print(f"Total number of columns: {df.shape[1]}")

# Check for missing values
print(df.isnull().sum())

import seaborn as sns

# Distribution of emotions
sns.countplot(data=df, x='high_level_emotion')
plt.title("Distribution of Emotions")
plt.show()

# Summing up each FACS code column to see their distribution
FACS_CODES = ['AU17', 'AU1', 'AU2', 'AU25', 'AU27', 'AU4', 'AU7', 'AU23', 'AU24', 'AU6', 'AU12', 'AU15', 'AU14', 'AU11', 'AU26']
facs_counts = df[FACS_CODES].sum()

# Plotting
facs_counts.plot(kind='bar', figsize=(12, 6))
plt.title("Distribution of FACS Codes")
plt.ylabel("Number of images")
plt.xlabel("FACS Code")
plt.show()

# Checking class imbalances for each label
for code in FACS_CODES:
    print(f"{code} - {df[code].mean():.2%}")

import matplotlib.image as mpimg

folder_path = '/content/drive/MyDrive/dataset/'

def show_sample_images(df, emotion, num_images=5):
    sample_df = df[df['high_level_emotion'] == emotion].sample(num_images)
    plt.figure(figsize=(15, 5))
    for idx, (index, row) in enumerate(sample_df.iterrows()):
        img_path = os.path.join(folder_path, row['filepath'])
        img = mpimg.imread(img_path)
        plt.subplot(1, num_images, idx+1)
        plt.imshow(img, cmap='gray')
        plt.title(row['high_level_emotion'])
        plt.axis('off')
    plt.show()

# Displaying sample images for each emotion
for emotion in df['high_level_emotion'].unique():
    show_sample_images(df, emotion)

# Image processing and data augmentation setup
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define constants
IMG_SIZE = 160
BATCH_SIZE = 32
NUM_FACS_CODES = len(FACS_CODES)

# Create a combined labels column
for df in [train_df, val_df, test_df]:
  df['combined_labels'] = df.apply(lambda row: [row['high_level_emotion']] + [row[facs] for facs in FACS_CODES], axis=1)

# Set up the ImageDataGenerators
train_datagen = ImageDataGenerator(
        rescale=1./255,
        width_shift_range=.2,
        height_shift_range=.2,
        rotation_range=30,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        validation_split=0.15)

train_generator = train_datagen.flow_from_dataframe(
    train_df,
    directory=folder_path,
    x_col='filepath',
    y_col='combined_labels',
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='raw',
    shuffle=True,
    subset='training',
    color_mode='rgb'
)

# Data generator for validation data
val_datagen = ImageDataGenerator(rescale=1./255)

validation_generator = train_datagen.flow_from_dataframe(
    val_df,
    directory=folder_path,
    x_col='filepath',
    y_col='combined_labels',
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='raw',
    subset='validation',
    shuffle=True,
    color_mode='rgb'
)

# Data generator for test data
test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_dataframe(
    test_df,
    directory=folder_path,
    x_col='filepath',
    y_col='combined_labels',
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='raw',
    shuffle=False,
    color_mode='rgb'
)

# Custom data generator for feeding data into the model
class DataGenerator(tf.keras.utils.Sequence):
    def __init__(self, df, batch_size, num_classes_emotion, num_classes_facs, shuffle=True, augment=None):
        self.df = df
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.augment = augment
        self.indexes = np.arange(len(self.df))
        self.num_classes_emotion = num_classes_emotion
        self.num_classes_facs = num_classes_facs

        if self.shuffle:
            np.random.shuffle(self.indexes)

    def __len__(self):
        return int(np.ceil(len(self.df) / self.batch_size))

    def __getitem__(self, index):
        start = index * self.batch_size
        end = (index + 1) * self.batch_size
        batch_data = self.df[start:end]

        X, y1, y2 = self.__data_generation(batch_data)

        return X, [y1, y2]

    def __data_generation(self, batch_data):
        images = []
        emotion_labels = []
        facs_labels = []

        for _, row in batch_data.iterrows():
            image_path = os.path.join(folder_path, row['filepath'])
            image = load_img(image_path, target_size=(160, 160))
            image = img_to_array(image)
            image = preprocess_input(image)

            # One-hot encode the emotion label
            emotion = to_categorical(row['high_level_emotion'], num_classes=self.num_classes_emotion)

            facs = row[FACS_CODES].values
            facs = facs.astype(np.float32)  # Convert to float32

            images.append(image)
            emotion_labels.append(emotion)
            facs_labels.append(facs)

        if self.augment:
            images = self.augment(images)

        return np.array(images), np.array(emotion_labels), np.array(facs_labels)

    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.indexes)

# Preprocessing: Label encoding for 'high_level_emotion' and combining labels
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from keras.applications.imagenet_utils import preprocess_input
from tensorflow.keras.utils import to_categorical

NUM_CLASSES_EMOTION = df['high_level_emotion'].nunique()
NUM_CLASSES_FACS = len(FACS_CODES)

# Fit the LabelEncoder and transform the 'high_level_emotion' column for each dataframe
label_encoder = LabelEncoder()

train_df['high_level_emotion'] = label_encoder.fit_transform(train_df['high_level_emotion'])
val_df['high_level_emotion'] = label_encoder.fit_transform(val_df['high_level_emotion'])
test_df['high_level_emotion'] = label_encoder.fit_transform(test_df['high_level_emotion'])

# Create the 'combined_labels' column for each dataframe
for dataframe in [train_df, val_df, test_df]:
    dataframe['combined_labels'] = dataframe.apply(lambda row: [row['high_level_emotion']] + [row[facs] for facs in FACS_CODES], axis=1)

# Now, create the data generators
train_gen = DataGenerator(train_df, batch_size=BATCH_SIZE, num_classes_emotion=NUM_CLASSES_EMOTION, num_classes_facs=NUM_CLASSES_FACS, shuffle=True, augment=None)
val_gen = DataGenerator(val_df, batch_size=BATCH_SIZE, num_classes_emotion=NUM_CLASSES_EMOTION, num_classes_facs=NUM_CLASSES_FACS, shuffle=False, augment=None)

# Setting up the model architecture using MobileNetV2 as the base model
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.regularizers import l2

# Load the MobileNetV2 model pre-trained on ImageNet data
base_model = MobileNetV2(input_shape=(160, 160, 3), include_top=False, weights='imagenet')

for layer in base_model.layers:
    layer.trainable = False

from tensorflow.keras.layers import Flatten, Dense, Dropout
from tensorflow.keras.models import Model


# Freeze the base model (you can also fine-tune it if required)
base_model.trainable = False

# Use the base model's output and add your custom layers
x = base_model.output

# Convert feature map to vectors
x = Flatten()(x)

# Add a dense layer with regularization and batch normalization
x = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)

# Define the two output layers
output_emotion = Dense(NUM_CLASSES_EMOTION, activation='softmax', name='emotion_output', kernel_regularizer=l2(0.01))(x)
output_facs = Dense(NUM_CLASSES_FACS, activation='sigmoid', name='facs_output', kernel_regularizer=l2(0.01))(x)

# Construct the full model
model = Model(inputs=base_model.input, outputs=[output_emotion, output_facs])

steps_per_epoch = len(train_generator) // BATCH_SIZE,
validation_steps = len(validation_generator) // BATCH_SIZE,

# Model compilation and training
model.compile(optimizer='adam',
              loss={'emotion_output': 'categorical_crossentropy', 'facs_output': 'binary_crossentropy'},
              metrics=['accuracy'])

model.fit(train_gen, validation_data=val_gen, steps_per_epoch=len(train_gen), validation_steps=len(val_gen), epochs=10)

# Unfreeze some layers of the base model for fine-tuning and train again
for layer in base_model.layers[-20:]:
    layer.trainable = True

# Recompile the model with a smaller learning rate
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss={'emotion_output': 'categorical_crossentropy', 'facs_output': 'binary_crossentropy'},
              metrics=['accuracy'])

model.fit(train_gen, validation_data=val_gen, steps_per_epoch=len(train_gen), validation_steps=len(val_gen), epochs=10)

# Evaluating the model's performance on the test set
test_gen = DataGenerator(test_df, batch_size=BATCH_SIZE, num_classes_emotion=NUM_CLASSES_EMOTION, num_classes_facs=NUM_CLASSES_FACS, shuffle=False, augment=None)
results = model.evaluate(test_gen, steps=len(test_gen))
print("Emotion Loss:", results[1])
print("Emotion Accuracy:", results[3])
print("FACS Loss:", results[2])
print("FACS Accuracy:", results[4])

# Fetch a batch of images and labels
X_val_batch, [y_val_emotion, y_val_facs] = val_gen[0]

# Get the model's predictions
predictions = model.predict(X_val_batch)
predicted_emotions, predicted_facs = predictions

# Function to visualise predictions on a batch of validation images
def plot_predictions(images, true_emotions, true_facs, pred_emotions, pred_facs, label_encoder, num_images=5):
    plt.figure(figsize=(15, 5 * num_images))
    for i in range(num_images):
        plt.subplot(num_images, 2, 2 * i + 1)
        plt.imshow(images[i])
        plt.title(f"Actual Emotion: {label_encoder.inverse_transform([np.argmax(true_emotions[i])])[0]}\n"
                  f"Predicted Emotion: {label_encoder.inverse_transform([np.argmax(pred_emotions[i])])[0]}")

        plt.subplot(num_images, 2, 2 * i + 2)
        plt.imshow(images[i])
        true_facs_labels = [FACS_CODES[j] for j in range(len(FACS_CODES)) if true_facs[i][j] > 0.5]
        predicted_facs_labels = [FACS_CODES[j] for j in range(len(FACS_CODES)) if pred_facs[i][j] > 0.5]

        plt.title(f"Actual FACS: {', '.join(true_facs_labels)}\n"
                  f"Predicted FACS: {', '.join(predicted_facs_labels)}")
    plt.tight_layout()
    plt.show()

plot_predictions(X_val_batch, y_val_emotion, y_val_facs, predicted_emotions, predicted_facs, label_encoder)

!pip install keras-tuner

# Hyperparameter tuning using keras-tuner
from tensorflow.keras.layers import Input
from kerastuner import HyperModel

class EmotionHyperModel(HyperModel):
    def __init__(self, input_shape, num_classes_emotion, num_classes_facs):
        self.input_shape = input_shape
        self.num_classes_emotion = num_classes_emotion
        self.num_classes_facs = num_classes_facs

    def build(self, hp):
        base_model = MobileNetV2(input_shape=self.input_shape, include_top=False, weights='imagenet')
        for layer in base_model.layers:
            layer.trainable = False

        x = base_model.output
        x = Flatten()(x)
        x = Dense(units=hp.Int('units', min_value=32, max_value=512, step=32, default=128),
                  activation='relu',
                  kernel_regularizer=l2(hp.Float('l2_value', min_value=1e-4, max_value=1e-1, sampling='LOG', default=1e-2)))(x)
        x = BatchNormalization()(x)
        x = Dropout(rate=hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1, default=0.5))(x)

        output_emotion = Dense(self.num_classes_emotion, activation='softmax', name='emotion_output')(x)
        output_facs = Dense(self.num_classes_facs, activation='sigmoid', name='facs_output')(x)

        model = Model(inputs=base_model.input, outputs=[output_emotion, output_facs])

        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default=1e-3)),
                      loss={'emotion_output': 'categorical_crossentropy', 'facs_output': 'binary_crossentropy'},
                      metrics=['accuracy'])

        return model

# Running random search hyperparameter tuning
from kerastuner.tuners import RandomSearch
import kerastuner as kt

hypermodel = EmotionHyperModel(input_shape=(160, 160, 3), num_classes_emotion=NUM_CLASSES_EMOTION, num_classes_facs=NUM_CLASSES_FACS)

tuner = RandomSearch(
    hypermodel,
    objective=kt.Objective('val_emotion_output_accuracy', direction='max'),
    max_trials=5,  # number of hyperparameter combinations to try
    executions_per_trial=1,
    directory='random_search',
    project_name='emotion_recognition'
)

# Search for the best hyperparameters
tuner.search(train_gen, epochs=10, validation_data=val_gen)

# Training the best model obtained from hyperparameter tuning
best_model = tuner.get_best_models(num_models=1)[0]

tuner.results_summary()

# Recompile the model with a tuned hyperparameters and metrics
from tensorflow.keras.metrics import Precision, Recall, AUC
from sklearn.metrics import hamming_loss

# Define custom metrics
precision = Precision(name='precision')
recall = Recall(name='recall')
auc = AUC(name='auc')

best_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss={'emotion_output': 'categorical_crossentropy', 'facs_output': 'binary_crossentropy'},
              metrics=['accuracy', precision, recall, auc])

best_model.fit(train_gen, validation_data=val_gen, steps_per_epoch=len(train_gen), validation_steps=len(val_gen), epochs=10)

# Evaluating the best model's performance on the test set
test_gen = DataGenerator(test_df, batch_size=BATCH_SIZE, num_classes_emotion=NUM_CLASSES_EMOTION, num_classes_facs=NUM_CLASSES_FACS, shuffle=False, augment=None)
results = best_model.evaluate(test_gen, steps=len(test_gen))

# Printing the results
print("Emotion Loss:", results[1])
print("Emotion Accuracy:", results[3])
print("Emotion Precision:", results[4])
print("Emotion Recall:", results[5])
print("Emotion AUC:", results[6])
print("FACS Loss:", results[2])
print("FACS Accuracy:", results[7])
print("FACS Precision:", results[8])
print("FACS Recall:", results[9])
print("FACS AUC:", results[10])

# Fetch a batch of images and labels
X_val_batch, [y_val_emotion, y_val_facs] = val_gen[0]

# Get the best model's predictions
predictions = best_model.predict(X_val_batch)
predicted_emotions, predicted_facs = predictions

# Visualising best model's predictions on validation images
def plot_predictions(images, true_emotions, true_facs, pred_emotions, pred_facs, label_encoder, num_images=5):
    plt.figure(figsize=(15, 5 * num_images))
    for i in range(num_images):
        plt.subplot(num_images, 2, 2 * i + 1)
        plt.imshow(images[i])
        plt.title(f"Actual Emotion: {label_encoder.inverse_transform([np.argmax(true_emotions[i])])[0]}\n"
                  f"Predicted Emotion: {label_encoder.inverse_transform([np.argmax(pred_emotions[i])])[0]}")

        plt.subplot(num_images, 2, 2 * i + 2)
        plt.imshow(images[i])
        true_facs_labels = [FACS_CODES[j] for j in range(len(FACS_CODES)) if true_facs[i][j] > 0.5]
        predicted_facs_labels = [FACS_CODES[j] for j in range(len(FACS_CODES)) if pred_facs[i][j] > 0.5]

        plt.title(f"Actual FACS: {', '.join(true_facs_labels)}\n"
                  f"Predicted FACS: {', '.join(predicted_facs_labels)}")
    plt.tight_layout()
    plt.show()

plot_predictions(X_val_batch, y_val_emotion, y_val_facs, predicted_emotions, predicted_facs, label_encoder)