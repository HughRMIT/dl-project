# -*- coding: utf-8 -*-
"""A2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zf70tK_kHjT8_Jc5EQdgqVMYe9Eun6jj

# Assignment 2: Deep Learning Project
**COSC2779 - Deep Learning - 2023 - Hugh Hancock - s3840125**

In this assignment, you will design and create an end-to-end deep learning system for a real-world problem. This assignment is designed for you to apply and practice skills of critical analysis and evaluation to circumstances similar to those found in real-world problems.

In this assignment you will:

- Design and Create an end-to-end deep learning system.
- Analyse and Evaluate the output of the algorithms.
- Research into extending techniques that are taught in class.
- Provide an ultimate judgement of the final trained model(s) that you would use in a real-world setting.

![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)  This notebook is designed to run on Google Colab. If you like to run this on your local machine, make sure that you have installed TensorFlow version 2.0.

Note: Code taken from RMIT Canvas, Learning Materials/Activities and Labs.

## Setting up the Notebook

Let's first load the packages we need.
"""

import tensorflow as tf
print(tf.__version__)
AUTOTUNE = tf.data.experimental.AUTOTUNE
import numpy as np
import pandas as pd

import tensorflow_datasets as tfds
import pathlib
import shutil
import tempfile

from  IPython import display
from matplotlib import pyplot as plt

"""We can use the tensor board to view the learning curves. Let's first set it up."""

# Commented out IPython magic to ensure Python compatibility.
logdir = pathlib.Path(tempfile.mkdtemp())/"tensorboard_logs"
shutil.rmtree(logdir, ignore_errors=True)

# Load the TensorBoard notebook extension
# %load_ext tensorboard

# Open an embedded TensorBoard viewer
# %tensorboard --logdir {logdir}/models

"""We can also write our own function to plot the models training history ones training has completed.

"""

from itertools import cycle
def plotter(history_hold, metric = 'binary_crossentropy', ylim=[0.0, 1.0]):
  cycol = cycle('bgrcmk')
  for name, item in history_hold.items():
    y_train = item.history[metric]
    y_val = item.history['val_' + metric]
    x_train = np.arange(0,len(y_val))

    c=next(cycol)

    plt.plot(x_train, y_train, c+'-', label=name+'_train')
    plt.plot(x_train, y_val, c+'--', label=name+'_val')

  plt.legend()
  plt.xlim([1, max(plt.xlim())])
  plt.ylim(ylim)
  plt.xlabel('Epoch')
  plt.ylabel(metric)
  plt.grid(True)

"""## Loading Dataset

The original dataset is from “Semeval-2016 Task 6: Detecting Stance in Tweets. Saif
M. Mohammad et. al. In Proceedings of the International Workshop on Semantic Eval-
uation (SemEval-16). June 2016.” The data set is available on canvas. This data set can
be combined with other data sets that you might obtain from the internet to improve
performance.
"""

import csv

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/My Drive/StanceDataset/train.csv'

rows = []
with open(file_path, 'r', encoding='ISO-8859-1') as file:
    reader = csv.reader(file)
    for row in reader:
        rows.append(row)

# Convert rows to DataFrame
df = pd.DataFrame(rows[1:], columns=rows[0])
print(df.head())

"""## Data cleaning and pre processing


"""

# Convert stance to integers
encode = {
    'FAVOR': 0,
    'AGAINST': 1,
    'NONE': 2,
}

df["Stance"] = df["Stance"].apply(lambda x: encode[x])
df.head()

"""### Text cleaning

Cleaning text is an important part in NLP and involves lot of engineering. The code below uses python string package and nltk to do some cleaning.
"""

import string
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

tweet_lines = list()
tweets = df["Tweet"].values.tolist()

for line in tweets:
    # tokenize the text
    tokens = word_tokenize(line)

    # convert to lower case
    tokens = [w.lower() for w in tokens]

    # remove puntuations
    table = str.maketrans('', '', string.punctuation)
    stripped = [w.translate(table) for w in tokens]

    # remove non alphabetic characters
    words = [word for word in stripped if word.isalpha()]

    tweet_lines.append(words)

target_lines = list()
targets = df["Target"].values.tolist()

for target in targets:
    # tokenize the target
    tokens = word_tokenize(target)

    # convert to lower case
    tokens = [w.lower() for w in tokens]

    # remove punctuations
    table = str.maketrans('', '', string.punctuation)
    stripped = [w.translate(table) for w in tokens]

    # remove non alphabetic characters
    words = [word for word in stripped if word.isalpha()]

    target_lines.append(words)

tweet_lines[:2]

target_lines[:2]

combined_lines = [target + tweet for target, tweet in zip(target_lines, tweet_lines)]

"""### Exploring the dataset

Let’s explore the dataset to get a better understanding. First, let's print out a random data point in the train set.
"""

# Splitting features and labels
tweets = df['Tweet'].values
stances = df['Stance'].values
targets = df['Target'].values

# Convert to TensorFlow dataset
train_dataset = tf.data.Dataset.from_tensor_slices((tweets, stances, targets))

# To shuffle and take a sample:
for x, y, z in train_dataset.shuffle(100).take(1):
    print('The string: "{}"'.format(x.numpy().decode()))
    print('The label: {}'.format(y.numpy()))
    print('The target: {}'.format(z.numpy().decode()))

"""### Generating the task vocabulary

You can use the keras `Tokenizer` to generate a vocabulary for your data
"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

validation_split = 0.20
max_length = 55

# Define and fit the tokenizer on the tweets
tokenizer = Tokenizer()
tokenizer.fit_on_texts(combined_lines)
sequences = tokenizer.texts_to_sequences(combined_lines)

word_index = tokenizer.word_index
print("unique tokens - "+str(len(word_index)))
vocab_size = len(tokenizer.word_index) + 1
print('vocab_size - '+str(vocab_size))

lines_pad = pad_sequences(sequences, maxlen=max_length, padding='post')
category =  df['Stance'].values

indices = np.arange(lines_pad.shape[0])
np.random.shuffle(indices)
lines_pad = lines_pad[indices]
category = category[indices]

n_values = np.max(category) + 1
Y = np.eye(n_values)[category]

num_validation_samples = int(validation_split * lines_pad.shape[0])

X_train_pad = lines_pad[:-num_validation_samples]
y_train = Y[:-num_validation_samples]
X_val_pad = lines_pad[-num_validation_samples:]
y_val = Y[-num_validation_samples:]

# Randomly sample some train data
train_len = X_train_pad.shape[0]

idx = np.random.randint(train_len, size=train_len//25)

X_train_pad_sampled = X_train_pad[idx, :]
y_train_sampled = y_train[idx]

print('Shape of X_train_pad:', X_train_pad.shape)
print('Shape of y_train:', y_train.shape)

print('Shape of X_train_pad_sampled:', X_train_pad_sampled.shape)
print('Shape of y_train_sampled:', y_train_sampled.shape)

print('Shape of X_test_pad:', X_val_pad.shape)
print('Shape of y_test:', y_val.shape)

"""## No transfer learning

First lets try a simple model without transfer learning.
"""

def get_callbacks(name):
  return [
    tf.keras.callbacks.TensorBoard(logdir/name, histogram_freq=1),
  ]

from tensorflow.keras.layers import Dense, Embedding, GRU, LSTM, Bidirectional
from tensorflow.keras.models import Sequential

embedding_dim = 300

embedding_layer = Embedding(len(word_index) + 1,
                            embedding_dim,
                            input_length=max_length,
                            trainable=True)

model_glove = Sequential()
model_glove.add(embedding_layer)
model_glove.add(LSTM(units=32,  dropout=0.2, recurrent_dropout=0.25))
model_glove.add(Dense(3, activation='softmax'))

model_glove.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])

print(model_glove.summary())

EPOCH = 10

m_histories = {}
m_histories['no_TL'] = model_glove.fit(X_train_pad_sampled, y_train_sampled, batch_size=32, epochs=EPOCH, validation_data=(X_val_pad, y_val), callbacks=get_callbacks('models/no_TL'), verbose=1)

plotter(m_histories, ylim=[0.0, 2.0], metric = 'loss')

plotter(m_histories, ylim=[0.0, 1.1], metric = 'categorical_accuracy')

"""## With Transfer Learning

Now lets explore how we can transfere the word embeddings. We will be using the GloVe word embeddings made avaialbe stanford reserchers: [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)

I have downloaded the 300 dimentional embedding matrix trained on Wikipedia.
"""

!cp /content/drive/MyDrive/glove.6B.300d.txt .

file = open('glove.6B.300d.txt', encoding='utf-8')

glove_vectors = dict()
for line in file:
  values = line.split()
  word = values[0]
  features = np.asarray(values[1:])
  glove_vectors[word] = features

file.close()

embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))
for word, i in word_index.items():
    embedding_vector = glove_vectors.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

from tensorflow.keras.layers import Dense, Embedding, GRU, LSTM, Bidirectional, Dropout
from tensorflow.keras.models import Sequential

embedding_layer_TL = Embedding(len(word_index) + 1,
                            embedding_dim,
                            weights=[embedding_matrix],
                            input_length=max_length,
                            trainable=False)

model_glove = Sequential()
model_glove.add(embedding_layer_TL)
model_glove.add(Bidirectional(LSTM(units=32, dropout=0.2, recurrent_dropout=0.25)))
model_glove.add(Dense(64, activation='relu'))
model_glove.add(Dropout(0.5))
model_glove.add(Dense(3, activation='softmax'))

model_glove.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])

print(model_glove.summary())

m_histories['with_TL'] = model_glove.fit(X_train_pad, y_train, batch_size=32, epochs=EPOCH, validation_data=(X_val_pad, y_val), callbacks=get_callbacks('models/with_TL'), verbose=1)

plotter(m_histories, ylim=[0.0, 2.0], metric = 'loss')

plotter(m_histories, ylim=[0.0, 1.1], metric = 'categorical_accuracy')

!pip install keras-tuner

import tensorflow as tf
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout
from tensorflow.keras.models import Sequential
from keras_tuner import RandomSearch
from tensorflow.keras import regularizers

def build_model(hp):
    model = Sequential()
    model.add(embedding_layer_TL)
    model.add(Bidirectional(LSTM(
        units=hp.Int('lstm_units', min_value=16, max_value=128, step=16),
        dropout=hp.Float('lstm_dropout', min_value=0.1, max_value=0.5, step=0.1),
        recurrent_dropout=hp.Float('lstm_recurrent_dropout', min_value=0.1, max_value=0.5, step=0.1),
        kernel_regularizer=regularizers.l2(hp.Float('lstm_l2_reg', min_value=1e-4, max_value=1e-1, sampling='LOG')),
        recurrent_regularizer=regularizers.l2(hp.Float('lstm_rec_l2_reg', min_value=1e-4, max_value=1e-1, sampling='LOG'))
    )))
    model.add(Dense(
        units=hp.Int('dense_units', min_value=32, max_value=128, step=16),
        activation='relu',
        kernel_regularizer=regularizers.l2(hp.Float('dense_l2_reg', min_value=1e-4, max_value=1e-1, sampling='LOG'))
    ))
    model.add(Dropout(
        rate=hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)
    ))
    model.add(Dense(3, activation='softmax'))

    model.compile(optimizer=tf.keras.optimizers.Adam(
        hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),
        loss='categorical_crossentropy',
        metrics=['categorical_accuracy']
    )
    return model

tuner = RandomSearch(
    build_model,
    objective='val_categorical_accuracy',
    max_trials=1,
    executions_per_trial=1,
    directory='output',
    project_name='model_glove_tuning'
)

tuner.search_space_summary()

tuner.search(X_train_pad, y_train, epochs=10, validation_data=(X_val_pad, y_val))

# Retrieve the best model and hyperparameters:
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
best_model = tuner.hypermodel.build(best_hps)

best_model_history = best_model.fit(
    X_train_pad, y_train,
    epochs=10,
    validation_data=(X_val_pad, y_val),
    callbacks=get_callbacks('models/with_TL'),
    verbose=1
)

test_file_path = '/content/drive/My Drive/StanceDataset/test.csv'

test_rows = []
with open(test_file_path, 'r', encoding='ISO-8859-1') as file:
    reader = csv.reader(file)
    for row in reader:
        test_rows.append(row)

# Convert rows to DataFrame
test_df = pd.DataFrame(test_rows[1:], columns=test_rows[0])

test_df["Stance"] = test_df["Stance"].apply(lambda x: encode[x])
test_category = test_df['Stance'].values
y_test_encoded = np.eye(n_values)[test_category]

test_lines = list()
test_tweets = test_df["Tweet"].values.tolist()

for line in test_tweets:
    tokens = word_tokenize(line)
    tokens = [w.lower() for w in tokens]
    table = str.maketrans('', '', string.punctuation)
    stripped = [w.translate(table) for w in tokens]
    words = [word for word in stripped if word.isalpha()]
    test_lines.append(words)

# Use the same tokenizer as the training data
test_sequences = tokenizer.texts_to_sequences(test_lines)

X_test_pad = pad_sequences(test_sequences, maxlen=max_length, padding='post')

predictions = best_model.predict(X_test_pad)

predicted_labels = np.argmax(predictions, axis=1)

# Evaluate the model
loss, accuracy = best_model.evaluate(X_test_pad, y_test_encoded)
print(f"Test set loss: {loss:.4f}, accuracy: {accuracy:.4f}")

from sklearn.metrics import classification_report, confusion_matrix

# If y_test_encoded is one-hot, convert to integer labels
if y_test_encoded.ndim == 2:
    y_test_labels = np.argmax(y_test_encoded, axis=1)
else:
    y_test_labels = y_test_encoded

print(confusion_matrix(y_test_labels, predicted_labels))
print(classification_report(y_test_labels, predicted_labels))